{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf5c5893-8bed-4115-87dc-50ebc5cacbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "''' 이동 평균 계산 함수 '''\n",
    "def running_mean(x, N=50):\n",
    "    kernel = np.ones(N) # N개의 1로 채워진 1차원 ndarray\n",
    "    conv_len = x.shape[0] - N # N개의 데이터 씩 이동 평균을 구하기 위해 몇 번째 데이터에서 반복을 멈춰야 하는지 결정한다 \n",
    "    y = np.zeros(conv_len + 1) # conv_len + 1개의 0으로 채워진 1차원 ndarray\n",
    "\n",
    "    # M개의 데이터를 N개 씩 묶어 이동 평균을 계산했을 때 산출되는 이동 평균의 개수\n",
    "    # = M - N + 1\n",
    "    for i in range(conv_len + 1): # +1을 해주어야 마지막 데이터까지 이동 평균 계산에 활용할 수 있다\n",
    "        y[i] = kernel @ x[i:i+N] # @는 행렬곱을 의미한다\n",
    "        y[i] /= N # 이동 평균 계산\n",
    "\n",
    "    return y\n",
    "\n",
    "''' 할인된 보상 계산 함수 '''\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    len_r = len(rewards) # 에피소드에서 받은 보상의 개수\n",
    "    # time step이 에피소드의 끝(에피소드의 패배)에 다가갈 수록 해당 time step이 에피소드의 패배에 어느 정도 관여한다고 가정한다\n",
    "    disc_return = torch.pow(gamma, torch.arange(len_r).float()) * rewards # 할인된 보상 계산\n",
    "    disc_return /= disc_return.max() # 할인된 보상을 [0, 1] 사이의 값으로 정규화한다(이는 모델 학습 안정성(혹은 수치적 안정성)을 위한 조치이다)\n",
    "\n",
    "env = gym.make(\"CartPole-v1\") # 카트폴 환경 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4b234e-a097-4874-943f-13f2e1b7fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 4 # 입력층 노드의 개수\n",
    "l2 = 150 # 첫번째 은닉층 노드의 개수\n",
    "l3 = 2 # 출력층 노드의 개수 \n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(l1, l2),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(l2, l3),\n",
    "    nn.Softmax(dim=0) # 확률분포 계산을 위해 사용되었다\n",
    ")\n",
    "\n",
    "lr = 0.009\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "def loss_fn(preds, r):\n",
    "    # 에피소드 내의 각 행동에 대해 (할인된 보상이 곱해진)음의 로그 확률을 계산한 후, 그 총합을 loss로 한다\n",
    "    return -1 * torch.sum(r * torch.log(preds)) # 이 때의 r은 \"감마 X 총 수익\" 계산이 완료된 할인된 보상이다\n",
    "\n",
    "\n",
    "''' 모델 학습 '''\n",
    "MAX_DUR = 200 # 에피소드 최대 길이(최대 time step 수)\n",
    "MAX_EPISODES = 500 # 총 에피소드 수(= epoch)\n",
    "gamma = 0.99 # 할인율\n",
    "score = [] # 에피소드의 길이를 점수로 한다\n",
    "expectation = 0.0\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    curr_state = env.reset()[0] # (상태, {})에서 상태만 가져온다\n",
    "    done = False # 게임 종료 여부\n",
    "    transitions = [] # 에피소드 내의 경험(상태, 행동, 보상)을 저장한다\n",
    "\n",
    "    for t in range(MAX_DUR):\n",
    "        act_prob = model(torch.from_numpy(curr_state).float()) # 현재 상태에서 취할 수 있는 행동들에 대한 확률분포를 반환한다\n",
    "        action = np.random.choice(np.array[0, 1], p = act_prob.data.numpy()) # 두 개의 행동(좌로 이동, 우로 이동) 중 하나를 선택한다\n",
    "        prev_state = curr_state\n",
    "        curr_state, _, done, _, _ = env.step(action) # 선택한 행동을 취하여 다음 상태로 전이한다\n",
    "        transitions.append((prev_state, action, t+1)) # 경험(현재 상태, 현재 상태에서 취한 행동, 그에 따른 보상) 생성\n",
    "        if done: # 게임이 종료되면 반복을 중단한다\n",
    "            break\n",
    "\n",
    "    ep_len = len(transitions) # 에피소드의 길이를 계산한다\n",
    "    score.append(ep_len) # 점수 추가\n",
    "    reward_batch = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0f372c17-d4bb-40cb-b49b-84be87b8b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([-0.02617598, -0.02468133,  0.03085263, -0.02335671], dtype=float32), {}) <class 'tuple'>\n",
      "type of state1[0]:  <class 'numpy.ndarray'>\n",
      "type of state1[1]:  <class 'dict'>\n",
      "reward:  1.0 , type of reward:  <class 'float'>\n",
      "done:  False , type of done:  <class 'bool'>\n",
      "info:  False\n",
      "what:  {}\n"
     ]
    }
   ],
   "source": [
    "state1 = env.reset() # 환경 초기화\n",
    "print(state1, type(state1))\n",
    "print(\"type of state1[0]: \", type(state1[0])) # 상태는 ndarray로 표현된다\n",
    "print(\"type of state1[1]: \",type(state1[1]))\n",
    "\n",
    "pred = model(torch.from_numpy(state1[0]))\n",
    "action = np.random.choice(np.array([0, 1]), p=pred.data.numpy())\n",
    "state2, reward, done, info, what = env.step(action) # done = 게임 종료 여부, info = 디버깅을 위한 정보\n",
    "print(\"reward: \", reward, \", type of reward: \", type(reward))\n",
    "print(\"done: \", done, \", type of done: \", type(done))\n",
    "print(\"info: \", info)\n",
    "print(\"what: \", what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12d7d561-8efc-4f4e-8e66-26ddd6f8503c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th reward: 1.0\n",
      "0th done: False\n",
      "1th reward: 1.0\n",
      "1th done: False\n",
      "2th reward: 1.0\n",
      "2th done: False\n",
      "3th reward: 1.0\n",
      "3th done: False\n",
      "4th reward: 1.0\n",
      "4th done: False\n",
      "5th reward: 1.0\n",
      "5th done: False\n",
      "6th reward: 1.0\n",
      "6th done: False\n",
      "7th reward: 1.0\n",
      "7th done: False\n",
      "8th reward: 1.0\n",
      "8th done: False\n",
      "9th reward: 1.0\n",
      "9th done: False\n",
      "10th reward: 1.0\n",
      "10th done: False\n",
      "11th reward: 1.0\n",
      "11th done: False\n",
      "12th reward: 1.0\n",
      "12th done: False\n",
      "13th reward: 1.0\n",
      "13th done: False\n",
      "14th reward: 1.0\n",
      "14th done: False\n",
      "15th reward: 1.0\n",
      "15th done: False\n",
      "16th reward: 1.0\n",
      "16th done: False\n",
      "17th reward: 1.0\n",
      "17th done: False\n",
      "18th reward: 1.0\n",
      "18th done: True\n"
     ]
    }
   ],
   "source": [
    "cur_state = env.reset()[0] # 환경 초기화\n",
    "\n",
    "''' 카트폴 환경이 조금 이상한 것 같다\n",
    "    게임이 종료 되면(게임에서 패배하면) 1.0이 아닌 다른 보상을 받아야 하는데,\n",
    "    여기서는 게임이 종료되었음에도 1.0이 보상으로 반환된다'''\n",
    "for i in range(20):\n",
    "    # print(cur_state, cur_state[0], type(cur_state[0]))\n",
    "    pred = model(torch.from_numpy(cur_state).float())\n",
    "    action = np.random.choice(np.array([0, 1]), p=pred.data.numpy())\n",
    "    # 2번째 상태부터는 (상태 벡터, {})이 아니라 상태 벡터만 반환된다\n",
    "    cur_state, reward, done, info, _ = env.step(action) # done = 게임 종료 여부, info = 디버깅을 위한 정보\n",
    "\n",
    "    print(f\"{i}th reward: {reward}\")\n",
    "    print(f\"{i}th done: {done}\")\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
