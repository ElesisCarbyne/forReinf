{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd643c40-dae0-4c28-8b8b-04d9056bebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import gymnasium as gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(4, 25)\n",
    "        self.l2 = nn.Linear(25, 50)\n",
    "        self.actor_lin1 = nn.Linear(50, 2)\n",
    "        self.l3 = nn.Linear(50, 25)\n",
    "        self.critic_lin1 = nn.Linear(25, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim=0) # F.normalize()에서 도출되는 결과의 범위는 [-1.0, 1.0]이다\n",
    "        y = F.relu(self.l1(x))\n",
    "        y = F.relu(self.l2(y))\n",
    "        actor = F.log_softmax(self.actor_lin1(y), dim=0) # 음의 로그 확률 값을 모델 단에서 미리 계산한다(정확히는 그냥 로그 확률이다)\n",
    "        c = F.relu(self.l3(y.detach())) # 여기서 계산 그래프가 분리된다\n",
    "        critic = torch.tanh(self.critic_lin1(c)) # 가치 함수 값을 tanh을 사용하여 [-1.0, 1.0] 구간의 값으로 변환시켜준 것은\n",
    "                                                 # 이익 계산에서의 Returns를 정규화하여 [-1.0, 1.0] 구간의 값으로 변환하기 때문이다\n",
    "        return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4dc8bd-c772-43eb-a759-5f13d9c3fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' N단계 분산 이익 행위자-비평자 학습 '''\n",
    "def learner(t, worker_model, counter, params, lock):\n",
    "    worker_env = gym.make(\"CartPole-v1\") # 환경 불러오기\n",
    "    worker_opt = optim.Adam(lr=1e-4, params=worker_model.parameters())\n",
    "\n",
    "    for i in range(params[\"epochs\"]):\n",
    "        state_values, logprobs, rewards, G = run_episode_N(worker_env, worker_model, params[\"N_steps\"])\n",
    "        actor_loss, critic_loss, ep_len = update_params_N(worker_opt, state_values, logprobs, rewards, G)\n",
    "        with lock:\n",
    "            counter.value = counter.value + 1\n",
    "            print(counter.value)\n",
    "\n",
    "def run_episode_N(worker_env, worker_model, N_steps=10):\n",
    "    cur_state = torch.from_numpy(worker_env.reset()[0]).float()\n",
    "    state_values, logprobs, rewards = [], [], []\n",
    "    done = False\n",
    "    j = 0\n",
    "    G = torch.tensor([0]) # 반환값 계산에 부트스트래핑을 적용하기 위한 N 단계 상태의 상태 가치 함수 값을 저장한다\n",
    "    \n",
    "    while(j < N_steps and done == False):\n",
    "        j += 1\n",
    "        policy, state_value = worker_model(cur_state)\n",
    "        state_values.append(state_value)\n",
    "        logits = policy.view(-1) # 크기가 2인 1차원 텐서로 변환한다\n",
    "        action_dist = torch.distributions.categorical.Categorical(logits=logits) # 카테고리컬 분포는 시행 횟수 n이 1인 다항분포와 동일한 분포이다\n",
    "                                                                                 # 여기서의 역할은 주어진 로짓을 확률분포로 변환하여 이 확률분포를 토대로 표본을 추출할 수 있도록 하는 것이다\n",
    "        action = action_dist.sample()\n",
    "        logprob_ = logits[action]\n",
    "        logprobs.append(logprob_)\n",
    "        next_state, _, done, _, _ = worker_env.step(action.numpy())\n",
    "        cur_state = torch.from_numpy(next_state).float()\n",
    "        if done:\n",
    "            reward = -10 # N 단계 이전에 게임이 종료되었다면 에피소드 내의 전체 상태에 대한 반환값을 직접 계산할 수 있으므로,\n",
    "                         # 이 에피소드에 대해서는 반환값 계산에 G(상태 가치 함수값이자 반환값)를 사용하지 않는다는 의미에서 0값을 넣는다(그냥 위에서 초기화된 G를 반환하면 된다)\n",
    "        else:\n",
    "            reward = 1.0\n",
    "            G = state_value.detach() # 게임이 종료되지는 않았지만 에피소드가 N 단계까지 진행되었다면 N 단계의 상태에 대한 상태 가치 함수 값을 저장한다\n",
    "                                     # N 단계 상태에 대한 반환값은 N+1 단계 상태의 반환값을 구할 수 없으므로 계산할 수 없지만,\n",
    "                                     # N-1 단계 상태의 반환값은 그 상태에서 받은 보상값과 (다음 상태인)N 단계 상태에 대한 상태 가치 함수값(N 단계 상태의 반환값과 같은 것으로 간주된다)으로 계산할 수 있기 때문이다\n",
    "                                     # detach()를 해주어야 하는 이유는 이후에 G가 actor_loss 계산에 사용되는 Returns을 구하기 위해서 사용되는데 G는 비평자의 계산 그래프와 연결되어있다\n",
    "                                     # 만약 이를 그대로 사용하게 되면 actor_loss로 행위자와 더불어 비평자까지 같이 학습을 하게되므로 이를 방지하기 위해 G에 detach()를 해주어야 한다\n",
    "                                     # 즉, actor_loss가 비평자를 베제하고 행위자만을 학습시킬 수 있도록 하기 위함이다\n",
    "        rewards.append(reward)\n",
    "\n",
    "        if j == 1:\n",
    "            print(f\"about state_value: {state_value}, {state_value.size()}, {type(state_value)}\") # 크키가 1인 1차원 텐서이다\n",
    "            print(f\"about policy: {policy}, {policy.size()}, {type(policy)}\") # 크기가 2인 1차원 텐서이다\n",
    "            print(f\"about action: {action}, {action.size()}, {type(action)}\") # 스칼라이다\n",
    "        \n",
    "        return state_values, logprobs, rewards, G\n",
    "\n",
    "def update_params_N(worker_opt, state_values, logprobs, rewards, G, clc=0.1, gamma=0.95):\n",
    "    rewards = torch.tensor(rewards).flip(dims=(0,)).view(-1)\n",
    "    logprobs = torch.stack(logprobs).flip(dims=(0,)).view(-1) # torch.stack 대신 torch.tensor를 사용해도 된다\n",
    "    state_values = torch.stack(state_values).flip(dims=(0,)).view(-1) # torch.stack 대신 torch.tensor를 사용해도 된다\n",
    "    \n",
    "    Returns = [] # 반환값 저장, G가 비평자의 계산 그래프와 분리되었으므로 Returns 역시 비평자의 계산 그래프와 분리된다\n",
    "    ret_ = rewards[0] if G == 0 else G # 마지막 상태의 반환값을 계산한다\n",
    "                                         # G == 0 : 에피소드가 N 단계 상태에 도달하기 전에 종료되었을 경우, G != 0 : 에피소드가 N 단계 상태에 도달하여 종료되었을 경우\n",
    "    Returns.append(ret_)\n",
    "    for r in range(rewards.shape[0] - 1): # 마지막 상태를 제외한 보상의 개수만큼 반복을 수행한다\n",
    "        ret_ = rewards[r+1] + gamma * ret_  # 에피소드의 마지막 타임 스텝을 T라 했을 때, T-1 타임 스텝부터 반환값을 계산한다\n",
    "        Returns.append(ret_)\n",
    "\n",
    "    Returns = torch.stack(Returns).view(-1) # 텐서가 원소인 리스트를 torch.tensor를 통해 텐서로 변환하면 오류가 발생하는데, torch.stack을 사용하면 오류없이 텐서로 변환할 수 있다\n",
    "    Returns = F.normalize(Returns, dim=0) # 반환값들에 대해 정규화를 수행하여 [-1.0, 1.0] 구간의 값으로 변환한다\n",
    "                                          # 이것때문에 비평자의 출력에 tanh를 적용한 것이다\n",
    "    actor_loss = -1 * logprobs * (Returns - state_values.detach()) # 크기가 n인 1차원 텐서이다\n",
    "    critic_loss = torch.pow(state_values - Returns, 2) # 크기가 n인 1차원 텐서이다\n",
    "    loss = actor_loss.sum() + clc * critic_loss.sum() # 행위자가 비평자보다 더 빨리 학습하도록 하기 위해 clc=0.1을 곱한다\n",
    "                                                      # 비평자의 전체 손실 중 일부로만 역전파를 수행하여 비평자의 학습을 지연시킨다\n",
    "    \n",
    "    # 역전파 수행\n",
    "    worker_opt.zero_grad()\n",
    "    loss.backward()\n",
    "    worker_opt.step()\n",
    "\n",
    "    return actor_loss, critic_loss, len(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c879c2-a72f-4b1f-9ef3-d2ed461a5f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) torch.Size([1])\n",
      "tensor([1]) torch.Size([1])\n",
      "tensor([2]) torch.Size([1])\n",
      "tensor([True]) [1, 2, 3, 4, 5, 6]\n",
      "tensor([[0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([0, 1, 2]),\n",
       " tensor([[0, 1, 2]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' 실험 '''\n",
    "import torch\n",
    "\n",
    "a = torch.tensor([0])\n",
    "b = torch.tensor([1])\n",
    "c = torch.tensor([2])\n",
    "\n",
    "print(a, a.size())\n",
    "print(b, b.size())\n",
    "print(c, c.size())\n",
    "\n",
    "print(a == 0.0, list(range(1, 7)))\n",
    "print(torch.stack([a], dim=0))\n",
    "torch.stack([a, b, c], dim=0), torch.stack([a, b, c], dim=0).view(-1), torch.stack([a, b, c], dim=1), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47adee71-d080-4b4f-82aa-ab07ee6eb098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter.value in main-process:  6000\n",
      "sub-process's exitcode: '0\n"
     ]
    }
   ],
   "source": [
    "''' N단계 분산 이익 행위자-비평자 학습 '''\n",
    "import os\n",
    "import torch.multiprocessing as mp\n",
    "from lib.worker2 import worker\n",
    "import time as t\n",
    "\n",
    "'''\n",
    "    <참고>\n",
    "    https://devocean.sk.com/blog/techBoardDetail.do?ID=163669\n",
    "    https://realpython.com/python-gil/\n",
    "    \n",
    "    파이썬에서 생성되는 모든 객체들은 reference count variable(이하 \"참조 횟수 변수\"라 칭한다)를 갖는다\n",
    "    이는 해당 객체를 가리키는 참조자들의 수를 추적하여 메모리 관리를 하기 위함이다\n",
    "    어떤 객체의 참조 횟수가 0에 다다르면(참조 횟수 변수가 0값을 가지게 되면) 해당 객체가 차지하고 있는 메모리는 반환(회수)된다(즉, 객체가 소멸된다)\n",
    "    이렇듯 참조 횟수 변수는 매우 중요하기 때문에 정확히 추적되어야 하는데, 이를 위해 스레드 간 공유되는 모든 데이터 구조체들(객체)에 lock을 추가할 수 있다\n",
    "    하지만 (공유되는) 각각의 객체에 모두 lock을 추가한다는 것은 deadlock(교착 상태)을 야기하는 다중 lock 현상이 발생할 수 있다는 것을 의미한다\n",
    "    따라서 파이썬에서는 이를 해결하고자 단일 lock인 GIL을 도입하여 한번에 하나의 스레드만 실행할 수 있도록 만들었다\n",
    "    - 즉, 한 번에 하나의 스레드만 공유 자원들에 접근할 수 있게 강제한 것이다(이는 하나의 스레드가 모든 공유 자원을 선점하고 사용하는 것과 같다고 볼 수 있다)\n",
    "    GIL 덕분에 단일 스레드 프로그램은 그로 인한 성능상 혜택을 누릴 수 있지만, 다중 스레드 프로그램은 그렇지 못하고 되려 성능 저하가 발생한다\n",
    "    CPU 중심 다중 스레드 프로그램의 경우 특히나 GIL로 인해 병렬 처리가 불가능해져 작업 처리 지연으로 인한 성능 저하가 심하다\n",
    "    반면, I/O 중심 다중 스레드 프로그램의 경우 I/O 요청을 대기하는 데에 상당한 시간을 소비하므로(이 때 동안은 작업을 처리하지 않는다) 스레들끼리 GIL을 번갈아 가며 공유하는 형식이 되어 성능 저하가 거의 발생하지 않는다\n",
    "    GIL로 인한 성능 저하를 해결하기 위해 다중 프로세싱을 사용할 수 있다\n",
    "    다중 프로세싱을 사용하더라도 작업 처리 시간이 그에 비례하여 줄어드는 것은 아닌데, 이는 프로세스 관리 자체에 overhead(추가 비용)가 존재하기 때문이다\n",
    "    다중 프로세스는 다중 스레드보다 훨씬 더 무거운 작업이기 때문에 규모를 키우면 병목 현상이 발생하게 된다는 점을 명심해야 한다\n",
    "'''\n",
    "'''\n",
    "    <참고>\n",
    "    https://stackoverflow.com/questions/74635994/pytorchs-share-memory-vs-built-in-pythons-shared-memory-why-in-pytorch-we\n",
    "    https://superfastpython.com/multiprocessing-sharedmemory/\n",
    "    \n",
    "    python 내장 모듈인 multiprocessing이 공유 메모리(shared_memory)를 다루는 방법은 아래와 같다\n",
    "    1. 공유 메모리로부터 공유 객체를 생성한다\n",
    "    2. 다른 프로세스와 공유 객체를 공유할 때는 공유 메모리의 이름, 공유 객체의 데이터 크기(혹은 shape) 및 자료형을 serialize하여 전달한다\n",
    "    3. 다른 프로세스는 전달받은 것을 deserialize한 후, 이를 바탕으로 다시 공유 메모리로부터 공유 객체를 복원하여 사용한다\n",
    "    4. 공유 객체에 대한 변형은 그대로 공유 메모리에 반영되므로, 모든 프로세스가 동일한 영향을 받게 된다\n",
    "'''\n",
    "'''\n",
    "    여기서의 mp.Value는 python의 multiprocessing 모듈에 존재하는 클래스이다\n",
    "    torch.multiprocessing이 python 내장 모듈인 multiprocessing의 warraper이자 100% 호환되기 때문에 torch.multiprocessing에서도 mp.Value를 사용할 수 있는 것이다\n",
    "    mp.Value는 공유 메모리(shared memory)로부터 할당된(생성된) ctypes(C언어와 호환되는 자료형이다) 객체를 반환한다\n",
    "    기본적으로 mp.Value로 반환된 값은 실제로는 공유 메모리로부터 생성된 ctypes 객체에 대한 동기화된(lock 때문에 그렇다) wrapper이다(당연하게도 이 역시 객체이다)\n",
    "    생성된 ctypes 객체 자체는 mp.Value에서 반환된 객체(wrapper)의 value 속성을 통해 접근할 수 있다\n",
    "    \"i\"는 typecode_or_type의 인자로, 이것은 반환되는 객체의 자료형을 결정한다\n",
    "    - \"i\"는 C언어의 자료형으로는 \"signed int\"이고, python의 자료형으로는 \"int\"에 해당하며 최소 크기는 2바이트이다\n",
    "    typecode_or_type은 ctypes 형이나 또는 array 모듈에서 사용되는 종류의 단일 문자 typecode가 될 수 있다\n",
    "    \"+=\"와 같이 읽기와 쓰기를 함께 수반하는 연산들은 \"원자적(atomic) 연산\"을 지원하지 않는다\n",
    "    ==> 원자적 연산이란 여러 스레드 또는 프로세스에서 동시에 특정 데이터에 접근해도 해당 데이터의 일관성을 보장하는 연산을 말한다\n",
    "    따라서 공유 객체에 대해 증감 연산 같은 연산을 수행하고 싶다면, 해당 공유 객체를 선점하고 반환하는 과정(lock)을 반드시 명시해야 한다\n",
    "'''\n",
    "counter = mp.Value(\"i\", 0)\n",
    "sub_proc_num = 6\n",
    "model_file_name = \"test01_nL.p\"\n",
    "# model_file_name = \"test02_nL.p\"\n",
    "model_save_path = os.path.join(os.getcwd(), \"parameters\", model_file_name)\n",
    "\n",
    "params = {\n",
    "    \"epochs\":1000,\n",
    "    \"N_steps\":10\n",
    "}\n",
    "\n",
    "''' 프로세스 생성 및 실행 '''\n",
    "start = t.time()\n",
    "p = mp.Process(target=worker, args=(sub_proc_num, counter, params, model_save_path)) # args는 프로세스에게 할당할 작업의 인자를 의미한다\n",
    "# p.run() # p.run() 메서드를 호출하면 하위 프로세스를 통한 모델 학습 과정이 2번 반복된다(첫번째 학습 과정이 완료된 후 이어서 두번째 학습 과정이 시작된다)\n",
    "p.start() # 프로세스가 실행된다\n",
    "\n",
    "''' 하위 프로세스 작업 대기 및 종료 '''\n",
    "p.join() # join() 메서드가 호출된 프로세스가 작업을 끝마칠 때까지 대기한다(block)\n",
    "         # join() 메서드를 사용하지 않으면 자식 프로세스는 유휴상태(idle)에 들어가고 종료되지 않아(부모 프로세스는 종료된다) 좀비 프로세스가 되어 손수 kill해줘야만 소멸하게 된다\n",
    "         # 즉, join() 메서드가 하는 일은 부모 프로세스가 자식 프로세스보다 먼저 종료되지 못하도록 막는다\n",
    "\n",
    "print(\"counter.value in main-process: \", counter.value)\n",
    "print(f\"sub-process's running time: {(t.time() - start) / 60:.4f} min\")\n",
    "print(f\"sub-process's exitcode: '{p.exitcode}\") # 공유 객체에 저장된 값을 출력한다\n",
    "                                                # .exitcode는 자식 프로세스의 종료 코드(exit code)이다\n",
    "                                                # 자식 프로세스가 아직 종료되지 않았다면 \"None\"을 반환하고,\n",
    "                                                # 정상적으로 종료되었다면 \"0\"을 반환한다\n",
    "\n",
    "p.terminate() # 프로세스를 종료한다\n",
    "              # 부모 프로세스는 terminate() 메서드를 사용하지 않아도 자동으로 종료된다(하지만 자식 프로세스는 자동으로 종료되지 않는다)\n",
    "p.close() # 프로세스 객체를 해체하여 그것과 관련된 모든 자원들을 회수한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3532e7-51d2-4258-b460-df7bc7748d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from lib.model_testing import worker\n",
    "import os\n",
    "\n",
    "''' 학습 후 시험(test) '''\n",
    "dest_path = \"D:\\\\for_study\\\\workspace\\\\for_reinf\\\\tuorial\\\\parameters\"\n",
    "file_name = \"test01_nL.p\"\n",
    "# file_name = \"test02_nL.p\" # 의외로 잘 되기는 한다\n",
    "model_save_path = os.path.join(dest_path, file_name)\n",
    "\n",
    "''' 프로세스 생성 및 실행 '''\n",
    "p = mp.Process(target=worker, args=(model_save_path,))\n",
    "p.start()\n",
    "\n",
    "''' 프로세스 종료 및 자원 회수 '''\n",
    "p.join()\n",
    "p.terminate()\n",
    "p.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
